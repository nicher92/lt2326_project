{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at KB/bert-base-swedish-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, AutoTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification, BertModel\n",
    "\n",
    "device = torch.device('cuda:3')\n",
    "tokenizer = AutoTokenizer.from_pretrained('KB/bert-base-swedish-cased')\n",
    "bert = BertModel.from_pretrained('KB/bert-base-swedish-cased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing (no need to run again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all party values: [nan 'm' 's' 'kds' 'fp' 'c' 'v' '-' 'nyd' 'mp' 'kd' 'FP' 'C' 'M' 'S' 'MP'\n",
      " 'SD' 'V' 'KD' 'TALMANNEN' 'TREDJE VICE TALMANNEN' 'ANDRE VICE TALMANNEN'\n",
      " 'FÖRSTE VICE TALMANNEN' 'Andre vice talmannen' 'ÅLDERSPRESIDENTEN' 'L'\n",
      " 'HANS MAJESTÄT KONUNGEN' 'TJÄNSTGÖRANDE ÅLDERSPRESIDENTEN']\n",
      "number of speeches: 336139\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"speeches_as_csv_full.csv\")          #all speeches\n",
    "party_values = df.Party.unique()\n",
    "print(\"all party values: {}\".format(party_values))\n",
    "print(\"number of speeches: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all party values: ['m' 's' 'kds' 'fp' 'c' 'v' '-' 'nyd' 'mp' 'kd' 'FP' 'C' 'M' 'S' 'MP' 'SD'\n",
      " 'V' 'KD' 'TALMANNEN' 'TREDJE VICE TALMANNEN' 'ANDRE VICE TALMANNEN'\n",
      " 'FÖRSTE VICE TALMANNEN' 'Andre vice talmannen' 'ÅLDERSPRESIDENTEN' 'L'\n",
      " 'HANS MAJESTÄT KONUNGEN' 'TJÄNSTGÖRANDE ÅLDERSPRESIDENTEN']\n",
      "number of speeches: 333952\n"
     ]
    }
   ],
   "source": [
    "#remove speeches with no party label\n",
    "df = df.dropna()\n",
    "\n",
    "party_values = df.Party.unique()\n",
    "print(\"all party values: {}\".format(party_values))\n",
    "print(\"number of speeches: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all party values: ['m' 's' 'kd' 'fp' 'c' 'v' '-' 'nyd' 'mp' 'sd' 'TALMANNEN'\n",
      " 'TREDJE VICE TALMANNEN' 'ANDRE VICE TALMANNEN' 'FÖRSTE VICE TALMANNEN'\n",
      " 'Andre vice talmannen' 'ÅLDERSPRESIDENTEN' 'HANS MAJESTÄT KONUNGEN'\n",
      " 'TJÄNSTGÖRANDE ÅLDERSPRESIDENTEN']\n",
      "number of speeches: 333952\n"
     ]
    }
   ],
   "source": [
    "#kds = kristdemokratiska samlingspartiet = kd\n",
    "df = df.replace(\"FP\", \"fp\")\n",
    "df = df.replace(\"kds\", \"kd\")\n",
    "df = df.replace(\"SD\", \"sd\")\n",
    "df = df.replace(\"KD\", \"kd\")\n",
    "df = df.replace(\"C\", \"c\")\n",
    "df = df.replace(\"M\", \"m\")\n",
    "df = df.replace(\"S\", \"s\")\n",
    "df = df.replace(\"V\", \"v\")\n",
    "df = df.replace(\"MP\", \"mp\") \n",
    "df = df.replace(\"L\", \"fp\")            #folkpartiet changed name to liberalerna\n",
    "party_values = df.Party.unique()\n",
    "print(\"all party values: {}\".format(party_values))\n",
    "print(\"number of speeches: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all party values: ['m' 's' 'kd' 'fp' 'c' 'v' 'nyd' 'mp' 'sd']\n",
      "number of speeches: 332928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#non-party related rows to remove from df\n",
    "remove_indexes = ['-','TALMANNEN',\n",
    " 'TREDJE VICE TALMANNEN', 'ANDRE VICE TALMANNEN', 'FÖRSTE VICE TALMANNEN',\n",
    " 'Andre vice talmannen', 'ÅLDERSPRESIDENTEN', 'HANS MAJESTÄT KONUNGEN',\n",
    " 'TJÄNSTGÖRANDE ÅLDERSPRESIDENTEN']\n",
    "\n",
    "for item in remove_indexes:\n",
    "    toremove = df[df['Party'] == '{}'.format(item)].index\n",
    "    df.drop(toremove, inplace=True)\n",
    "\n",
    "party_values = df.Party.unique()\n",
    "print(\"all party values: {}\".format(party_values))\n",
    "print(\"number of speeches: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s      107247\n",
      "m       64593\n",
      "v       31474\n",
      "fp      30845\n",
      "c       29736\n",
      "mp      29710\n",
      "kd      27267\n",
      "sd      10981\n",
      "nyd      1075\n",
      "Name: Party, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#very unbalanced classes\n",
    "print(df[\"Party\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all party values: ['m' 's' 'kd' 'fp' 'c' 'v' 'mp' 'sd']\n",
      "number of speeches: 331853\n"
     ]
    }
   ],
   "source": [
    "#drop the nyd class because its too small\n",
    "toremove = df[df['Party'] == 'nyd'].index\n",
    "df.drop(toremove , inplace=True)\n",
    "\n",
    "party_values = df.Party.unique()\n",
    "print(\"all party values: {}\".format(party_values))\n",
    "print(\"number of speeches: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s     107247\n",
      "m      64593\n",
      "v      31474\n",
      "fp     30845\n",
      "c      29736\n",
      "mp     29710\n",
      "kd     27267\n",
      "sd     10981\n",
      "Name: Party, dtype: int64\n",
      "331829\n"
     ]
    }
   ],
   "source": [
    "print(df[\"Party\"].value_counts())\n",
    "print(len(df['Speech'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s     35036\n",
      "m     27032\n",
      "mp    12494\n",
      "v     10612\n",
      "c     10086\n",
      "fp     9334\n",
      "sd     8495\n",
      "kd     8267\n",
      "Name: Party, dtype: int64\n",
      "121353\n"
     ]
    }
   ],
   "source": [
    "#data for speeches between 2010-2021\n",
    "after_2010 = df[df['Date'].astype(str).str.startswith('201', '202')]\n",
    "print(after_2010[\"Party\"].value_counts())\n",
    "print(len(after_2010['Speech'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_2010.to_csv(\"speeches_as_csv_2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189541</th>\n",
       "      <td>189541</td>\n",
       "      <td>fp</td>\n",
       "      <td>Fru talman! Luciano Astudillo beskriver de ol...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189542</th>\n",
       "      <td>189542</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Först har jag några reflexioner k...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189543</th>\n",
       "      <td>189543</td>\n",
       "      <td>fp</td>\n",
       "      <td>Fru talman! Tillbaka till frågan om lots: Det...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189544</th>\n",
       "      <td>189544</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Jag tror inte att det tjänar syft...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189545</th>\n",
       "      <td>189545</td>\n",
       "      <td>fp</td>\n",
       "      <td>Fru talman! Jag håller med Luciano Astudillo ...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189546</th>\n",
       "      <td>189546</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Klimatkrisen accelererar. De sena...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189547</th>\n",
       "      <td>189547</td>\n",
       "      <td>fp</td>\n",
       "      <td>Fru talman! Det är riktigt, som Anders Ygeman...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189548</th>\n",
       "      <td>189548</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Jag är glad att vi har denna disk...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189549</th>\n",
       "      <td>189549</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Låt mig först göra en reflexion ö...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189550</th>\n",
       "      <td>189550</td>\n",
       "      <td>fp</td>\n",
       "      <td>Fru talman! Anders Ygeman är uppenbart besvär...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189551</th>\n",
       "      <td>189551</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Det är inte Sverige som bygger ko...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189552</th>\n",
       "      <td>189552</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Sydafrikas regering bär naturligt...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189553</th>\n",
       "      <td>189553</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Jag sade att jag välkomnade debat...</td>\n",
       "      <td>2010-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189554</th>\n",
       "      <td>189554</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Utskottet har varit enigt om det ...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189555</th>\n",
       "      <td>189555</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Låt mig få bemöta det första Per ...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189556</th>\n",
       "      <td>189556</td>\n",
       "      <td>v</td>\n",
       "      <td>Fru talman! Du hakar upp dig på polisen. Om d...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189557</th>\n",
       "      <td>189557</td>\n",
       "      <td>c</td>\n",
       "      <td>Fru talman! Det blev så mycket där på slutet ...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189558</th>\n",
       "      <td>189558</td>\n",
       "      <td>kd</td>\n",
       "      <td>Fru talman! De rödgröna vill inte gärna vidgå...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189559</th>\n",
       "      <td>189559</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Jag tror inte att det blir en sär...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189560</th>\n",
       "      <td>189560</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Jag har ingenting annat i sak att...</td>\n",
       "      <td>2010-05-05 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Party                                             Speech  \\\n",
       "189541      189541    fp   Fru talman! Luciano Astudillo beskriver de ol...   \n",
       "189542      189542     s   Fru talman! Först har jag några reflexioner k...   \n",
       "189543      189543    fp   Fru talman! Tillbaka till frågan om lots: Det...   \n",
       "189544      189544     s   Fru talman! Jag tror inte att det tjänar syft...   \n",
       "189545      189545    fp   Fru talman! Jag håller med Luciano Astudillo ...   \n",
       "189546      189546     s   Fru talman! Klimatkrisen accelererar. De sena...   \n",
       "189547      189547    fp   Fru talman! Det är riktigt, som Anders Ygeman...   \n",
       "189548      189548     m   Fru talman! Jag är glad att vi har denna disk...   \n",
       "189549      189549     s   Fru talman! Låt mig först göra en reflexion ö...   \n",
       "189550      189550    fp   Fru talman! Anders Ygeman är uppenbart besvär...   \n",
       "189551      189551     m   Fru talman! Det är inte Sverige som bygger ko...   \n",
       "189552      189552     s   Fru talman! Sydafrikas regering bär naturligt...   \n",
       "189553      189553     m   Fru talman! Jag sade att jag välkomnade debat...   \n",
       "189554      189554     m   Fru talman! Utskottet har varit enigt om det ...   \n",
       "189555      189555     m   Fru talman! Låt mig få bemöta det första Per ...   \n",
       "189556      189556     v   Fru talman! Du hakar upp dig på polisen. Om d...   \n",
       "189557      189557     c   Fru talman! Det blev så mycket där på slutet ...   \n",
       "189558      189558    kd   Fru talman! De rödgröna vill inte gärna vidgå...   \n",
       "189559      189559     m   Fru talman! Jag tror inte att det blir en sär...   \n",
       "189560      189560     s   Fru talman! Jag har ingenting annat i sak att...   \n",
       "\n",
       "                       Date  \n",
       "189541  2010-04-30 00:00:00  \n",
       "189542  2010-04-30 00:00:00  \n",
       "189543  2010-04-30 00:00:00  \n",
       "189544  2010-04-30 00:00:00  \n",
       "189545  2010-04-30 00:00:00  \n",
       "189546  2010-04-30 00:00:00  \n",
       "189547  2010-04-30 00:00:00  \n",
       "189548  2010-04-30 00:00:00  \n",
       "189549  2010-04-30 00:00:00  \n",
       "189550  2010-04-30 00:00:00  \n",
       "189551  2010-04-30 00:00:00  \n",
       "189552  2010-04-30 00:00:00  \n",
       "189553  2010-04-30 00:00:00  \n",
       "189554  2010-05-05 00:00:00  \n",
       "189555  2010-05-05 00:00:00  \n",
       "189556  2010-05-05 00:00:00  \n",
       "189557  2010-05-05 00:00:00  \n",
       "189558  2010-05-05 00:00:00  \n",
       "189559  2010-05-05 00:00:00  \n",
       "189560  2010-05-05 00:00:00  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run from here\n",
    "\n",
    "#after_2010 = pd.read_csv(\"speeches_as_csv_2010.csv\")\n",
    "after_2010[1000:1020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kd    20981\n",
      "fp    20981\n",
      "c     20981\n",
      "mp    20981\n",
      "v     20981\n",
      "s     20981\n",
      "m     20981\n",
      "sd    10981\n",
      "Name: Party, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#takes number, dataframe, returns new dataframe with more even classes (length of minimum class + number)\n",
    "def new_dataframe(number, dataframe):\n",
    "    \n",
    "    \n",
    "    #groups dataframes based on party, \n",
    "    grouped_parties = []\n",
    "    for party, df_party in dataframe.groupby('Party'):\n",
    "        grouped_parties.append(df_party)\n",
    "    \n",
    "    #max number of instances of each party\n",
    "    maxlength = min([len(x) for x in grouped_parties]) + number\n",
    "\n",
    "    #new dataframe gets max number of parties \n",
    "    #adjusted according to the smallest occuring class to even distribution\n",
    "    new_df = pd.DataFrame()\n",
    "    for item in grouped_parties:\n",
    "        item = item[:maxlength]\n",
    "\n",
    "        new_df = new_df.append(item)\n",
    "\n",
    "    #shuffles the data\n",
    "    new_df = shuffle(new_df)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "new_df = new_dataframe(10000, df)           \n",
    "#new_df = new_dataframe(3000, after_2010)\n",
    "\n",
    "new_df[500:520]\n",
    "print(new_df[\"Party\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19481</th>\n",
       "      <td>19481</td>\n",
       "      <td>s</td>\n",
       "      <td>Fru talman! Med anledning av det som Stig Sand...</td>\n",
       "      <td>1995-02-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251342</th>\n",
       "      <td>251342</td>\n",
       "      <td>sd</td>\n",
       "      <td>&lt;p&gt;Herr talman! Jag hör ju att regeringen och ...</td>\n",
       "      <td>2015-06-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223681</th>\n",
       "      <td>223681</td>\n",
       "      <td>sd</td>\n",
       "      <td>Herr talman! Anders Andersson står och talar ...</td>\n",
       "      <td>2013-05-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43490</th>\n",
       "      <td>43490</td>\n",
       "      <td>mp</td>\n",
       "      <td>Fru talman! Jag hade svårt att hålla mig från ...</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85268</th>\n",
       "      <td>85268</td>\n",
       "      <td>m</td>\n",
       "      <td>Fru talman! Tack för svaret, finansministern! ...</td>\n",
       "      <td>2001-03-16 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0 Party                                             Speech  \\\n",
       "19481        19481     s  Fru talman! Med anledning av det som Stig Sand...   \n",
       "251342      251342    sd  <p>Herr talman! Jag hör ju att regeringen och ...   \n",
       "223681      223681    sd   Herr talman! Anders Andersson står och talar ...   \n",
       "43490        43490    mp  Fru talman! Jag hade svårt att hålla mig från ...   \n",
       "85268        85268     m  Fru talman! Tack för svaret, finansministern! ...   \n",
       "\n",
       "                       Date  \n",
       "19481   1995-02-09 00:00:00  \n",
       "251342  2015-06-15 00:00:00  \n",
       "223681  2013-05-22 00:00:00  \n",
       "43490   1997-02-26 00:00:00  \n",
       "85268   2001-03-16 00:00:00  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'sd': 1, 'mp': 2, 'm': 3, 'c': 4, 'fp': 5, 'kd': 6, 'v': 7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Party</th>\n",
       "      <th>Speech</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19481</th>\n",
       "      <td>19481</td>\n",
       "      <td>0</td>\n",
       "      <td>Fru talman! Med anledning av det som Stig Sand...</td>\n",
       "      <td>1995-02-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251342</th>\n",
       "      <td>251342</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p&gt;Herr talman! Jag hör ju att regeringen och ...</td>\n",
       "      <td>2015-06-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223681</th>\n",
       "      <td>223681</td>\n",
       "      <td>1</td>\n",
       "      <td>Herr talman! Anders Andersson står och talar ...</td>\n",
       "      <td>2013-05-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43490</th>\n",
       "      <td>43490</td>\n",
       "      <td>2</td>\n",
       "      <td>Fru talman! Jag hade svårt att hålla mig från ...</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85268</th>\n",
       "      <td>85268</td>\n",
       "      <td>3</td>\n",
       "      <td>Fru talman! Tack för svaret, finansministern! ...</td>\n",
       "      <td>2001-03-16 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Party                                             Speech  \\\n",
       "19481        19481      0  Fru talman! Med anledning av det som Stig Sand...   \n",
       "251342      251342      1  <p>Herr talman! Jag hör ju att regeringen och ...   \n",
       "223681      223681      1   Herr talman! Anders Andersson står och talar ...   \n",
       "43490        43490      2  Fru talman! Jag hade svårt att hålla mig från ...   \n",
       "85268        85268      3  Fru talman! Tack för svaret, finansministern! ...   \n",
       "\n",
       "                       Date  \n",
       "19481   1995-02-09 00:00:00  \n",
       "251342  2015-06-15 00:00:00  \n",
       "223681  2013-05-22 00:00:00  \n",
       "43490   1997-02-26 00:00:00  \n",
       "85268   2001-03-16 00:00:00  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change party names to values\n",
    "party_values = new_df.Party.unique()\n",
    "\n",
    "counter = 0\n",
    "party_dict = {}\n",
    "for item in party_values:\n",
    "    party_dict[item] = counter\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "print(party_dict)\n",
    "new_df = new_df.replace({\"Party\": party_dict})\n",
    "new_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#turns each column in the dataframe into a list\n",
    "speeches = new_df.Speech.tolist()\n",
    "parties = new_df.Party.tolist()\n",
    "dates = new_df.Date.tolist()\n",
    "\n",
    "\n",
    "#stratify takes even amount of parties in train/test, so there is no unbalance between classes in train/test\n",
    "train_data, test_data, label_train, label_test = train_test_split(speeches, parties, test_size=0.2, shuffle=True, stratify=parties)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#encodes all the data to format suitable for bert, returns list of that data\n",
    "\n",
    "def encoder(data):\n",
    "    list_of_data = []\n",
    "    for text in data:\n",
    "        encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,                 #start and stop token for sequences\n",
    "        truncation = True,                       #truncated\n",
    "        max_length=512,                          #maxlength is maximum allowable length, 512\n",
    "        return_token_type_ids=False,             \n",
    "        padding=\"max_length\",                    #pad to max length\n",
    "        return_attention_mask=True,              #attention only on non padded\n",
    "        return_tensors='pt',                     #pytorch tensors\n",
    "        ).to(device)\n",
    "        list_of_data.append(encoding)           \n",
    "    \n",
    "    return list_of_data\n",
    "\n",
    "\n",
    "\n",
    "train_data = encoder(train_data)\n",
    "test_data = encoder(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  7172, 26201,   421,   361, 17274,    43,   654,   408,  7323,\n",
       "          1081,     7,   160,    54, 11742,   246,    48,  3737,    48,  1254,\n",
       "          8289,    36,   108,  5868, 18556,     7,  1362,   217,  4060,  1797,\n",
       "            82,  1696,  2980,  3498,    31,    97,  1924,  1974,    19,  1686,\n",
       "            48,    82,   686,  1245, 11777,   195,   100,  2971,     6,     7,\n",
       "           160,    54,  1061,  1344,    67, 12201, 21963,   620,   127,   108,\n",
       "          1276, 39242,   603,   945,  1864, 12948, 28404,     7, 39249,   198,\n",
       "           894,    68, 12205,   108,   217,    59,  1756,   100, 20734,    65,\n",
       "            97,     7,   361,   108,  9359,   118, 14869,   146,  8855,    19,\n",
       "            36,   327,   724,   198,    48, 10792, 11819,    54, 11128,    36,\n",
       "          1109, 28152,     7,   160,    54,  1413,   351,  7683,    43,    97,\n",
       "         13002,    67,   186,   346, 12952,     7,   160,  3292,   522,   100,\n",
       "            97,   561,   549,   535,   256,     7,   839,   100, 16041,    43,\n",
       "         12205,    54,    48, 24118,   100,  1864, 12948,  5640,    54,   217,\n",
       "           127,  1127,    68,    48, 10792,  1765,  9610,   603,    82,  1736,\n",
       "             7,  1571,   178,   430,  1532, 11601,    68,    82,   264,  3676,\n",
       "         20095,   549,  7762,  2765,     7,  4892, 20442,    48,   198,   108,\n",
       "           829, 22308,     7,   730, 12278,  4918,   108,   186,  2778,   203,\n",
       "         31799,    82,   230,    82,   945, 15864,   416,  1200,    19,    76,\n",
       "          1377,    31,  1485,    67,   217,   397,   146,     7,   820, 14981,\n",
       "          1864,  5640, 11501,   203,   686, 10900,    19,   146, 10619,  1031,\n",
       "            76,  1109, 28152,    36, 11128,     7,   160,   523,    19,   216,\n",
       "            82, 19933,    59,  1783,  1640, 35688,    74,     7,   833,   178,\n",
       "          9121,    48,   928,   151,     7,   133,     7, 24299,  2716,    59,\n",
       "          1205,   100,   102,   433, 22308,    42,     7,   371,  1772,    48,\n",
       "         10619,  3837,    96,  7663,   146,  1109, 28152,    36, 11128,    36,\n",
       "            48,   654,   509,   264,  1063,    96,    43, 18715,     7,   160,\n",
       "           423, 21781,  1144,   146, 11741, 30953, 20632,    67,  7705,    68,\n",
       "            48,   825,  6136,   108,  7996,    31,  1061,  1332,   166,     7,\n",
       "           219, 47817, 49796,   928,    68,  2427,    65,  6909,    52,  1453,\n",
       "            19,    36,    97,    54,  1767,   127,  1990,  1197,     7,   160,\n",
       "            54,  1854,  2836,    48,   198,  1053,  1326,    31,   654,     7,\n",
       "             3,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], device='cuda:3'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:3')}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  7172, 26201,   421,   361,   178,  1947,    66,   100,    48,\n",
       "            82,    54,  1500,    48,  6269,    59,  4094,     7,   361,   108,\n",
       "           382,  1523,   680,    19,    67,   217,   178,  2785, 18007, 46128,\n",
       "            19,  1413,  5194,    36,   408,  8024,    18,  1465,    71,    52,\n",
       "           743,   612,  8964,  8111,   512,   358, 29668, 49792,     7,   361,\n",
       "           178,   127,  1765,  3253,    48,   198,   512,  4836,   358, 24744,\n",
       "            33,   100,   137,  8538, 11178,    67,   423, 23048,    43,   440,\n",
       "         23432,    52,   102,     7,   135,  2568,   398,   217,  1616,   230,\n",
       "            82,   945,   102,     8,     7,    26,     7, 25916,   711,    52,\n",
       "         49547,  6782,   185,    19,    67,   178,   358,   137, 28960, 15825,\n",
       "          6974, 31281, 49796,    52,  2066,    43,  3534,    19,    48,    82,\n",
       "         13560,   945,   871,  2260,    43,   559,    67,    43,   413,   664,\n",
       "          2087,     7,   335,   512, 31013,    68,   871,   692,    67,  8964,\n",
       "          8111,     7,   160,   512,   358,    59,  1251,  9600, 11321,    65,\n",
       "           559,    19,    66,   871,  2934,    65, 32018, 27981,    67,  1320,\n",
       "         32975,   147,    19,  1385, 49816,   715,   493,    19,  5239,    19,\n",
       "          1137, 12613, 49792,    19, 22894,    36,   440,   102, 19522,    42,\n",
       "             7,   160,   397,   127,    48,  9012,  3930,  2260,    68,    97,\n",
       "          8843,    65,  2087,     7,   135,  2568,   178,   217, 48386,    66,\n",
       "         22208,  3736,    31,    48,   186,  2531,   393,  3270,    48, 23757,\n",
       "            76,    97,   382, 12695,    19,   653,    59,  2953,  1735,    19,\n",
       "            43,    48,  6643,   671, 17456,    76,  4732,     7,   361,  2288,\n",
       "          2214,    43,    52,  1339,   251,    48,    82,   512,   248,    59,\n",
       "           509, 11124,    36,    68,  8538, 30317,  7811,     7,     3,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], device='cuda:3'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:3')}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists of labels and data for both train and test\n",
    "\n",
    "train_list = []\n",
    "for i in range(len(label_train)):\n",
    "    to_append = label_train[i], train_data[i]\n",
    "    train_list.append(to_append)\n",
    "\n",
    "test_list = []\n",
    "for i in range(len(label_test)):\n",
    "    to_append = label_test[i], test_data[i]\n",
    "    test_list.append(to_append)\n",
    "\n",
    "\n",
    "with open('test_data', 'wb') as f:\n",
    "    pickle.dump(test_list, f)\n",
    "\n",
    "with open('train_data', 'wb') as f:\n",
    "    pickle.dump(train_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def data_loader(trainingfile, testingfile, dataframe):\n",
    "    \n",
    "    with open(trainingfile, 'rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "\n",
    "    with open(testingfile, 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    \n",
    "    df = pd.read_csv(dataframe)       #only speeches from 2010\n",
    "    \n",
    "    return train_data, test_data, df\n",
    "\n",
    "\n",
    "#train_list, test_list, df = data_loader(\"train_data\", \"test_data\", \"speeches_as_csv_cleaned.csv\")                #all speeches\n",
    "train_list, test_list, df = data_loader(\"train_data_2010\", \"test_data_2010\", \"speeches_as_csv_2010.csv\")       #speeches after 2010\n",
    "\n",
    "\n",
    "party_values = df.Party.unique()\n",
    "counter = 0\n",
    "party_dict = {}\n",
    "for item in party_values:\n",
    "    party_dict[item] = counter\n",
    "    counter += 1\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#dataloaders for shuffling and batching\n",
    "\n",
    "train = DataLoader(train_list, shuffle=True, batch_size=16)\n",
    "test = DataLoader(test_list, shuffle=True, batch_size=16)\n",
    "\n",
    "device = torch.device('cuda:2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting token counts in text\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "token_counts = []\n",
    "for _, row in df.iterrows():\n",
    "    token_count = len(tokenizer.encode(\n",
    "    row[\"Speech\"],\n",
    "    max_length=4056,\n",
    "    truncation=True\n",
    "  ))\n",
    "    token_counts.append(token_count)\n",
    "\n",
    "sns.histplot(token_counts)\n",
    "plt.xlim([0, 4056]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "torch.Size([16, 1, 512])\n",
      "torch.Size([16, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "i1, l1 = next(iter(train))\n",
    "print(i1.shape)                           #labels for each item in batch\n",
    "print(l1[\"attention_mask\"].shape)         #attention mask, ie what to care about\n",
    "print(l1[\"input_ids\"].shape)              #input ids, ie translated numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze parameters, only fine tuning\n",
    "for param in bert.parameters(): \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, num_parties, bert):\n",
    "        super(BERT, self).__init__()   \n",
    "        self.bert = bert.to(device)\n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(256, num_parties)\n",
    "        self.sigmoid = nn.Sigmoid()          \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x, _ = self.bert(input_ids, attention_mask, return_dict=False) \n",
    "        \n",
    "        #all sentences, first token, all hidden state outputs\n",
    "        x = x[:,0,:]    #prediction on the cls token\n",
    "        \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is now 1.3853179138116163\n",
      "loss is now 1.361219477114366\n",
      "loss is now 1.351263716268699\n",
      "loss is now 1.3456875716235703\n",
      "loss is now 1.3418044345770666\n",
      "loss is now 1.3383281504819309\n",
      "loss is now 1.3357962967670698\n",
      "loss is now 1.3342102166260585\n",
      "loss is now 1.3325490292339888\n",
      "loss is now 1.3313135540670342\n",
      "loss is now 1.3302335293592378\n",
      "loss is now 1.3293649537350558\n",
      "loss is now 1.3277817212268883\n",
      "loss is now 1.3268472523221466\n",
      "loss is now 1.3260988539520941\n",
      "loss is now 1.3250659122792836\n",
      "loss is now 1.3244560384325872\n",
      "loss is now 1.3233873009963324\n",
      "loss is now 1.3228579199725292\n",
      "loss is now 1.3223167713610395\n",
      "loss is now 1.321718460239076\n",
      "loss is now 1.3209870341299554\n",
      "loss is now 1.3204247726985505\n",
      "loss is now 1.3200388418082745\n",
      "loss is now 1.319504023492694\n",
      "loss is now 1.3190615281101463\n",
      "loss is now 1.318613521884485\n",
      "loss is now 1.318104783177546\n",
      "loss is now 1.317569199359676\n",
      "loss is now 1.3169306902666416\n",
      "loss is now 1.3165246524871184\n",
      "loss is now 1.316200834376695\n",
      "loss is now 1.315784762337664\n",
      "loss is now 1.3156341922505879\n",
      "loss is now 1.315301744098594\n",
      "loss is now 1.315081112498739\n",
      "loss is now 1.3149067730750867\n",
      "loss is now 1.314565741433756\n",
      "loss is now 1.314358856329309\n",
      "loss is now 1.3138330260882767\n",
      "loss is now 1.3136134195921043\n",
      "loss is now 1.3132679092405692\n",
      "loss is now 1.3129846088087374\n",
      "loss is now 1.312646037937701\n",
      "loss is now 1.3125151464855123\n",
      "loss is now 1.312199932514364\n",
      "loss is now 1.312040116580072\n",
      "loss is now 1.311846582917254\n",
      "loss is now 1.3116589340111946\n",
      "loss is now 1.3114343695394706\n",
      "loss is now 1.3113059440905683\n",
      "loss is now 1.311027757819168\n",
      "loss is now 1.3108035288153561\n",
      "loss is now 1.310646384844053\n",
      "loss is now 1.310515613688905\n",
      "loss is now 1.310387931259791\n",
      "loss is now 1.3101321207857861\n",
      "loss is now 1.3100684285703497\n",
      "loss is now 1.3098075343029119\n",
      "loss is now 1.309640594737135\n",
      "loss is now 1.3094989711787979\n",
      "loss is now 1.3093057427652075\n",
      "loss is now 1.3090867097722403\n",
      "loss is now 1.3088894081867417\n",
      "loss is now 1.3086624544828667\n",
      "loss is now 1.3084778057402562\n",
      "loss is now 1.3083342329092498\n",
      "loss is now 1.3081481489105635\n",
      "loss is now 1.3079584674621634\n",
      "loss is now 1.3078068899186888\n",
      "loss is now 1.3076382450426711\n",
      "loss is now 1.3074348915499239\n",
      "loss is now 1.307244270960343\n",
      "loss is now 1.3071427747440074\n",
      "loss is now 1.3069917614427562\n",
      "loss is now 1.3067529405445029\n",
      "loss is now 1.3065514729590149\n",
      "loss is now 1.3063739101895768\n",
      "epoch is done\n",
      "loss is now 1.2939440151958754\n",
      "loss is now 1.2926016407246566\n",
      "loss is now 1.2918754054052775\n",
      "loss is now 1.289339124110707\n",
      "loss is now 1.2895970276159372\n",
      "loss is now 1.2896588559889037\n",
      "loss is now 1.2901543909430675\n",
      "loss is now 1.2911498767227045\n",
      "loss is now 1.290916249006821\n",
      "loss is now 1.290551829892952\n",
      "loss is now 1.2903555915830784\n",
      "loss is now 1.2903103899766843\n",
      "loss is now 1.290531322818889\n",
      "loss is now 1.2901135855504153\n",
      "loss is now 1.2899323557220832\n",
      "loss is now 1.2899002383059455\n",
      "loss is now 1.2899715688875522\n",
      "loss is now 1.290209400108816\n",
      "loss is now 1.2901460437664174\n",
      "loss is now 1.290308983192377\n",
      "loss is now 1.290420724571518\n",
      "loss is now 1.2906063612467487\n",
      "loss is now 1.290691671448202\n",
      "loss is now 1.2904498148282502\n",
      "loss is now 1.2905971128173044\n",
      "loss is now 1.2906237088656967\n",
      "loss is now 1.2907341388131832\n",
      "loss is now 1.2906611287655596\n",
      "loss is now 1.2905310853292877\n",
      "loss is now 1.2903923309616503\n",
      "loss is now 1.2903683781912343\n",
      "loss is now 1.2903464447803592\n",
      "loss is now 1.290186901378357\n",
      "loss is now 1.2902448125694317\n",
      "loss is now 1.2902167594902991\n",
      "loss is now 1.2899635853610723\n",
      "loss is now 1.29010259558878\n",
      "loss is now 1.2901864257312883\n",
      "loss is now 1.2902002435948612\n",
      "loss is now 1.290098782527235\n",
      "loss is now 1.2900048980598888\n",
      "loss is now 1.2899916991751148\n",
      "loss is now 1.2898927244766558\n",
      "loss is now 1.2899547803737976\n",
      "loss is now 1.2899035822519807\n",
      "loss is now 1.289889966856399\n",
      "loss is now 1.2897382965555189\n",
      "loss is now 1.289810927439218\n",
      "loss is now 1.2899300779498677\n",
      "loss is now 1.2897411663220104\n",
      "loss is now 1.2895993624663535\n",
      "loss is now 1.2895864397230046\n",
      "loss is now 1.2896169603045289\n",
      "loss is now 1.2895485645097633\n",
      "loss is now 1.2895148473965037\n",
      "loss is now 1.2894321490737195\n",
      "loss is now 1.2894573957120186\n",
      "loss is now 1.289448995200349\n",
      "loss is now 1.2893850000925358\n",
      "loss is now 1.2894262522980657\n",
      "loss is now 1.2893057232934895\n",
      "loss is now 1.2892378728405893\n",
      "loss is now 1.289213074383934\n",
      "loss is now 1.28912490781424\n",
      "loss is now 1.2890227883159977\n",
      "loss is now 1.2890012995083777\n",
      "loss is now 1.2890213076272712\n",
      "loss is now 1.2888881496220481\n",
      "loss is now 1.2888984685143554\n",
      "loss is now 1.2888454360410238\n",
      "loss is now 1.2887439961239626\n",
      "loss is now 1.2886818297407099\n",
      "loss is now 1.2886289212985926\n",
      "loss is now 1.2886267594649738\n",
      "loss is now 1.2886288720522743\n",
      "loss is now 1.2885600327386528\n",
      "loss is now 1.288471546445532\n",
      "loss is now 1.288398523858563\n",
      "epoch is done\n",
      "loss is now 1.2864039440949757\n",
      "loss is now 1.282421176083124\n",
      "loss is now 1.2811290716447161\n",
      "loss is now 1.2792587438248155\n",
      "loss is now 1.2800174300560732\n",
      "loss is now 1.2804645532260155\n",
      "loss is now 1.2811626730132342\n",
      "loss is now 1.2810390835187313\n",
      "loss is now 1.2811331087443667\n",
      "loss is now 1.2813087393333007\n",
      "loss is now 1.2809029463831134\n",
      "loss is now 1.2806244823488222\n",
      "loss is now 1.2802736760791766\n",
      "loss is now 1.2802073523519377\n",
      "loss is now 1.279813948643454\n",
      "loss is now 1.2799309575796276\n",
      "loss is now 1.279704737182643\n",
      "loss is now 1.2795818539345643\n",
      "loss is now 1.279678576177142\n",
      "loss is now 1.2793983851464883\n",
      "loss is now 1.27955021929633\n",
      "loss is now 1.2794682048311987\n",
      "loss is now 1.2791584772473057\n",
      "loss is now 1.2793432482459637\n",
      "loss is now 1.2793943570668147\n",
      "loss is now 1.2793182081536818\n",
      "loss is now 1.2795327447517661\n",
      "loss is now 1.2796174709436594\n",
      "loss is now 1.27960546396958\n",
      "loss is now 1.2796109702267937\n",
      "loss is now 1.27962004131523\n",
      "loss is now 1.279696888045506\n",
      "loss is now 1.2797181914614417\n",
      "loss is now 1.2797365039367823\n",
      "loss is now 1.279799869330347\n",
      "loss is now 1.279667011356314\n",
      "loss is now 1.279666858968718\n",
      "loss is now 1.2796355698666595\n",
      "loss is now 1.2795463664259903\n",
      "loss is now 1.2794947046083223\n",
      "loss is now 1.2795185517155272\n",
      "loss is now 1.2795372750715688\n",
      "loss is now 1.2795879567021629\n",
      "loss is now 1.2796706970265357\n",
      "loss is now 1.2796313090572413\n",
      "loss is now 1.2796930293916073\n",
      "loss is now 1.2797404179885858\n",
      "loss is now 1.2796307460488963\n",
      "loss is now 1.27970425773942\n",
      "loss is now 1.2796795189881425\n",
      "loss is now 1.2797703157981064\n",
      "loss is now 1.279803678625523\n",
      "loss is now 1.2797683479285822\n",
      "loss is now 1.2797176703813453\n",
      "loss is now 1.2797242533479913\n",
      "loss is now 1.2797653359517516\n",
      "loss is now 1.279781517087838\n",
      "loss is now 1.2798958466877957\n",
      "loss is now 1.279865318113898\n",
      "loss is now 1.2799185321944657\n",
      "loss is now 1.2798838023890977\n",
      "loss is now 1.2799259060562609\n",
      "loss is now 1.2799049973071501\n",
      "loss is now 1.2799841977829374\n",
      "loss is now 1.2799260246284119\n",
      "loss is now 1.279859921355142\n",
      "loss is now 1.2799206004825736\n",
      "loss is now 1.2799383298686926\n",
      "loss is now 1.280001258032359\n",
      "loss is now 1.2799920901104729\n",
      "loss is now 1.279922369828206\n",
      "loss is now 1.2799620528256899\n",
      "loss is now 1.2799532408358414\n",
      "loss is now 1.2799220592879108\n",
      "loss is now 1.2800359795063014\n",
      "loss is now 1.2800502321985712\n",
      "loss is now 1.2799677719275104\n",
      "loss is now 1.2798995930677133\n",
      "epoch is done\n",
      "loss is now 1.2736437547989565\n",
      "loss is now 1.274110113780702\n",
      "loss is now 1.2746522896564922\n",
      "loss is now 1.2731185668990725\n",
      "loss is now 1.2735929178869079\n",
      "loss is now 1.2741313426194087\n",
      "loss is now 1.2742419361599184\n",
      "loss is now 1.27468930986781\n",
      "loss is now 1.275057821802886\n",
      "loss is now 1.2751446319026154\n",
      "loss is now 1.274962234418429\n",
      "loss is now 1.275265519759673\n",
      "loss is now 1.2752880864849083\n",
      "loss is now 1.2757339777586032\n",
      "loss is now 1.2756443392085266\n",
      "loss is now 1.2755949483151134\n",
      "loss is now 1.2758216275194942\n",
      "loss is now 1.2759153727212174\n",
      "loss is now 1.2758999058389238\n",
      "loss is now 1.2761111719614866\n",
      "loss is now 1.276118416355132\n",
      "loss is now 1.2759261134945843\n",
      "loss is now 1.2757788574620401\n",
      "loss is now 1.275530994950905\n",
      "loss is now 1.2752109139668746\n",
      "loss is now 1.2752425263608222\n",
      "loss is now 1.2752456033823887\n",
      "loss is now 1.275184608396993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#had problem with overfitting, but seems to be fine after added dropout, small dropout, a large dropout didnt work well\n",
    "#also added relu\n",
    "\n",
    "\n",
    "model = BERT(num_parties=len(party_values), bert=bert)     #input values are number of parties and itself\n",
    "loss = nn.BCELoss()                                        #binary cross entropy loss between target and value\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "model.to(device)\n",
    "total_loss = 0\n",
    "epochs = 5\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "        \n",
    "    for param in bert.parameters(): \n",
    "            param.requires_grad = False\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(train):\n",
    "        \n",
    "        label = batch[0]\n",
    "        \n",
    "        a_label = label[0]\n",
    "\n",
    "        attn = batch[1][\"attention_mask\"].squeeze(1)       #was in wrong format (8, 1, 512) => should be (8, 512)?\n",
    "        inp = batch[1][\"input_ids\"].squeeze(1)             #was in wrong format (8, 1, 512) => should be (8, 512)?\n",
    "\n",
    "\n",
    "        label = label.to(device)\n",
    "        \n",
    "        attn = attn.to(device)\n",
    "        inp = inp.to(device)\n",
    "        \n",
    "        #turn to one hot representation to make it fit the loss function\n",
    "        one_hot_label = torch.nn.functional.one_hot(label, num_classes=len(party_values))\n",
    "        one_hot_label = one_hot_label.float()\n",
    "        \n",
    "        \n",
    "        out = model(input_ids = inp, attention_mask = attn)\n",
    "        \n",
    "        \n",
    "        loss_function = loss(out, one_hot_label).to(device)\n",
    "        total_loss += loss_function.item()\n",
    "        \n",
    "        # compute gradients\n",
    "        loss_function.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"loss is now {}\".format(total_loss / i + 1))           \n",
    "            #torch.save(model.state_dict(), \"model_bert_full\")    #save model each 100th step\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"epoch is done\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#have to redo old model, overwrote it wih model_bert_2010\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 16119, correct: 8387, accuracy: 52.03\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>(c,)</th>\n",
       "      <th>(fp,)</th>\n",
       "      <th>(kd,)</th>\n",
       "      <th>(m,)</th>\n",
       "      <th>(mp,)</th>\n",
       "      <th>(s,)</th>\n",
       "      <th>(sd,)</th>\n",
       "      <th>(v,)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>(c,)</th>\n",
       "      <td>555</td>\n",
       "      <td>28</td>\n",
       "      <td>119</td>\n",
       "      <td>59</td>\n",
       "      <td>30</td>\n",
       "      <td>61</td>\n",
       "      <td>69</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(fp,)</th>\n",
       "      <td>108</td>\n",
       "      <td>1414</td>\n",
       "      <td>146</td>\n",
       "      <td>211</td>\n",
       "      <td>246</td>\n",
       "      <td>77</td>\n",
       "      <td>285</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(kd,)</th>\n",
       "      <td>103</td>\n",
       "      <td>32</td>\n",
       "      <td>705</td>\n",
       "      <td>108</td>\n",
       "      <td>59</td>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(m,)</th>\n",
       "      <td>227</td>\n",
       "      <td>306</td>\n",
       "      <td>276</td>\n",
       "      <td>1499</td>\n",
       "      <td>108</td>\n",
       "      <td>88</td>\n",
       "      <td>117</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(mp,)</th>\n",
       "      <td>202</td>\n",
       "      <td>221</td>\n",
       "      <td>322</td>\n",
       "      <td>79</td>\n",
       "      <td>1425</td>\n",
       "      <td>160</td>\n",
       "      <td>389</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(s,)</th>\n",
       "      <td>171</td>\n",
       "      <td>38</td>\n",
       "      <td>188</td>\n",
       "      <td>45</td>\n",
       "      <td>153</td>\n",
       "      <td>1106</td>\n",
       "      <td>205</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(sd,)</th>\n",
       "      <td>92</td>\n",
       "      <td>100</td>\n",
       "      <td>89</td>\n",
       "      <td>31</td>\n",
       "      <td>123</td>\n",
       "      <td>94</td>\n",
       "      <td>876</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(v,)</th>\n",
       "      <td>196</td>\n",
       "      <td>114</td>\n",
       "      <td>172</td>\n",
       "      <td>221</td>\n",
       "      <td>109</td>\n",
       "      <td>93</td>\n",
       "      <td>124</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted  (c,)  (fp,)  (kd,)  (m,)  (mp,)  (s,)  (sd,)  (v,)\n",
       "Actual                                                       \n",
       "(c,)        555     28    119    59     30    61     69    86\n",
       "(fp,)       108   1414    146   211    246    77    285   106\n",
       "(kd,)       103     32    705   108     59    20     58    97\n",
       "(m,)        227    306    276  1499    108    88    117   274\n",
       "(mp,)       202    221    322    79   1425   160    389   189\n",
       "(s,)        171     38    188    45    153  1106    205   208\n",
       "(sd,)        92    100     89    31    123    94    876   100\n",
       "(v,)        196    114    172   221    109    93    124   807"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#confusion matrix\n",
    "def conf_mat(true, pred):\n",
    "\n",
    "    A = [tuple(x) for x in true]\n",
    "    B = [tuple(x) for x in pred]\n",
    "\n",
    "    X = pd.Series(A, name='Actual')\n",
    "    Y = pd.Series(B, name='Predicted')\n",
    "\n",
    "\n",
    "    df_confusion = pd.crosstab(X, Y)\n",
    "\n",
    "    return df_confusion\n",
    "\n",
    "\n",
    "#testing\n",
    "def testing(modelname, testing_data):\n",
    "\n",
    "    model = BERT(num_parties=len(party_values), bert=bert)\n",
    "    model.load_state_dict(torch.load(modelname)) #modelname\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "\n",
    "    for i, batch in enumerate(testing_data):\n",
    "        \n",
    "        \n",
    "        if modelname == \"model_bert_2010\":\n",
    "            _, batch = batch                                   #test data for 2010 have extra list, uncomment for testing\n",
    "\n",
    "        label = batch[0]\n",
    "        attn = batch[1][\"attention_mask\"].squeeze(1)       \n",
    "        inp = batch[1][\"input_ids\"].squeeze(1)             \n",
    "        label = label.to(device)\n",
    "        attn = attn.to(device)\n",
    "        inp = inp.to(device)\n",
    "\n",
    "\n",
    "        #do argmax with softmax\n",
    "        with torch.no_grad(): \n",
    "\n",
    "\n",
    "            out = model(input_ids = inp, attention_mask = attn)\n",
    "\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "\n",
    "            prediction = torch.argmax(out, dim=1)\n",
    "\n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(\"50 more done, total done {}\".format(i + 1), end=\"\\r\",)\n",
    "\n",
    "\n",
    "            predictions = [[k for k,v in party_dict.items() if v == x] for x in label]\n",
    "            correct = [[k for k,v in party_dict.items() if v == x] for x in prediction]\n",
    "            \n",
    "            \n",
    "            for j in range(len(predictions)):\n",
    "                y_pred.append(predictions[j])\n",
    "                y_true.append(correct[j])\n",
    "\n",
    "                if predictions[j] == correct[j]:\n",
    "\n",
    "                    correct_count += 1\n",
    "                    total_count += 1\n",
    "                else:\n",
    "                    total_count += 1\n",
    "                    \n",
    "                    \n",
    "    conf_full = conf_mat(y_true, y_pred)\n",
    "    return total_count, correct_count, y_true, y_pred, conf_full\n",
    "    \n",
    "    \n",
    "\n",
    "#all data tested\n",
    "#tot, cor, y_true, y_pred, confmat = testing(\"model_bert_full\", test)\n",
    "\n",
    "#2010-2020 data\n",
    "tot, cor, y_true, y_pred, confmat = testing(\"model_bert_2010\", test) #\n",
    "\n",
    "print(\"total: {}, correct: {}, accuracy: {:.2f}\".format(tot, cor, ((cor/tot) * 100)))\n",
    "confmat\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
